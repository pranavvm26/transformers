{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee07143-2c35-4547-b0f3-0b04d0438730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pranavvm26/transformers.git /tmp/transformers\n",
    "# !python3 -m pip install -e /tmp/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913be522-d403-4ad9-a17f-c1207f7ffa7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install -U torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6e192-5f94-4181-b2da-6810ea8794ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install scipy==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e920e-adc0-47b8-b6ce-78f99bcfc6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install -U bitsandbytes==0.43.0\n",
    "# !python3 -m pip install -U peft==0.8.1\n",
    "# !python3 -m pip install -U datasets==2.18.0 \n",
    "# !python3 -m pip install -U tensorboardX==2.6.2.2\n",
    "# !python3 -m pip install -U py7zr==0.21.0\n",
    "# !python3 -m pip install -U einops==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb0a4d-9c5d-4a4c-9d10-4232030af35f",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc59c83a-0031-48f8-b5ab-ce4278b36bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/greedy-lr-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training, \n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    AutoPeftModelForCausalLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b00dadf-29d3-4fb5-a1de-47b9162884c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF transformer version: 4.39.0.dev0.greedy\n"
     ]
    }
   ],
   "source": [
    "print(f\"HF transformer version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064cd59-ad97-4f8d-b433-539b70d0e64b",
   "metadata": {},
   "source": [
    "### To-do remove quantization, need raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f8f15e-300f-4141-8260-89d219d92ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [01:12<00:00, 36.02s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/greedy-lr-py310/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.51s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# The code is provided by the model authors in the repo.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    # quantization_config=bnb_config, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbc9cbd-4ac2-447c-abbe-a13d47a9a9da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Falcon tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe19f76f-beea-4342-9e59-9d1146159b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c5150f-4f8b-4196-b2e4-de5ed494dda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277f31b9-ebdc-4064-afe7-22db56b7352d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2088763392 || all params: 9010484096 || trainable%: 23.18147803986757\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=1024,\n",
    "    lora_alpha=3072,\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a49e2-9f50-4a93-8950-a83146c1cc3b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374ebe2f-6628-477e-ba98-7f8550cd826e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from random import randint\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241f7fe9-6e2e-4516-8c0b-0f452bec4a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"w601sxs/simpleCoT\" # \"databricks/databricks-dolly-15k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0db369d-5816-4806-9109-25c100c1e273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 427/427 [00:00<00:00, 3.39MB/s]\n",
      "Downloading data: 100%|██████████| 219M/219M [00:02<00:00, 77.1MB/s] \n",
      "Downloading data: 100%|██████████| 218M/218M [00:07<00:00, 30.2MB/s] \n",
      "Downloading data: 100%|██████████| 219M/219M [00:04<00:00, 49.4MB/s] \n",
      "Downloading data: 100%|██████████| 219M/219M [00:07<00:00, 31.0MB/s] \n",
      "Downloading data: 100%|██████████| 219M/219M [00:03<00:00, 61.4MB/s] \n",
      "Downloading data: 100%|██████████| 379M/379M [00:07<00:00, 51.5MB/s] \n",
      "Downloading data: 100%|██████████| 477M/477M [00:05<00:00, 80.2MB/s] \n",
      "Generating train split: 100%|██████████| 2214941/2214941 [00:08<00:00, 263996.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[:3000]\")\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[3000:3100]\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"train[3100:3200]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993e472d-4f7f-43a7-a4ab-a3c50313389c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3000\n",
      "Validation dataset size: 100\n",
      "Test dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea1d7293-3d73-461a-a912-d94aa031ebff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_simple_cot(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['source']}\"\n",
    "    answer = f\"### Answer\\n{sample['target']}\" \n",
    "    response = f\"### Rationale\\n{sample['rationale']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, answer, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26824bd6-ef16-49e8-8d43-de21bf04919e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 14313.73 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 10267.57 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 10458.57 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an abstract, generate a keyword (a noun phrase) that best describes the focus or contribution of the paper. Such keywords can be directly from the given abstract or outside it.\n",
      "\n",
      "Abstract: Small amounts (0.1-0.5 mM) of deoxycholate enhanced amylase secretion, which had been induced by submaximal doses of carbachol or cholecystokinin octapeptide, without affecting the maximal levels of these reactions from isolated rat pancreatic acini. Deoxycholate alone did not induce these reactions. The other bile acids such as cholate, chenodeoxycholate, ursodeoxycholate, and taurocholate were also active. Under the similar conditions, deoxycholate enhanced the secretagogue-induced diacylglycerol formation that was derived mainly from the phospholipase C-mediated hydrolysis of phosphatidylinositol and phosphatidylinositol-4-monophosphate. Deoxycholate did not enhance the secretagogue-induced hydrolysis of phosphatidylinositol-4,5-bisphosphate or Ca2+ mobilization. Deoxycholate did not affect amylase secretion, which was induced by the simultaneous addition of protein kinase C-activating 12-O-tetradecanoylphorbol-13-acetate and Ca2+ ionophore ionomycin. Since diacylglycerol and Ca2+ may be responsible for the secretagogue-induced amylase secretion, our results indicate that small amounts of bile acids increase the sensitivity to the secretagogue of diacylglycerol formation and subsequent activation of protein kinase C, and thereby enhance amylase secretion from pancreatic acini.\n",
      "\n",
      "### Answer\n",
      "Amylases\n",
      "\n",
      "### Rationale\n",
      "The abstract explains that small amounts of deoxycholate enhance amylase secretion, which is induced by submaximal doses of carbachol or cholecystokinin octapeptide. It also shows that the bile acids increase the sensitivity to the secretagogue of diacylglycerol formation and subsequent activation of protein kinase C, and thereby enhance amylase secretion from pancreatic acini. The research findings indicate that small amounts of bile acids can effectively improve amylases secretions in pancreas acini. Therefore, \"Amylases\" is a good keyword for this paper.<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_simple_cot(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "# train\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "# validation\n",
    "validation_dataset = validation_dataset.map(template_dataset, remove_columns=list(validation_dataset.features))\n",
    "# test\n",
    "test_dataset = test_dataset.map(template_dataset, remove_columns=list(test_dataset.features))\n",
    "\n",
    "# print random sample\n",
    "print(validation_dataset[randint(0, len(validation_dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    \n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6254d91-776c-4b35-b4d1-277aea1c874e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5297 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 4870.87 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 3335.38 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 4401.57 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3281.95 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3981.61 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2976.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length : 398 || Val Length : 12 || Test Length : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# validation\n",
    "lm_valid_dataset = validation_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(validation_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# validation\n",
    "lm_test_dataset = test_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(test_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Train Length : {len(lm_train_dataset)} || Val Length : {len(lm_valid_dataset)} || Test Length : {len(lm_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed3226-6ac8-4358-82de-35998f725295",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af2b25b-59ad-4a35-9eaa-89fdd7f86b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_schedulers = ['cosine', 'constant', 'greedy', 'linear']\n",
    "chosen_scheduler = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0207c5cb-5795-430d-becd-97ba398696cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging_dir = f\"./model-outputs/notebook/{lr_schedulers[chosen_scheduler]}/tensorboard\"\n",
    "output_dir = f\"./model-outputs/notebook/{lr_schedulers[chosen_scheduler]}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09500d01-b223-4980-980a-708c9814f29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/greedy-lr-py310/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=lm_train_dataset,\n",
    "    eval_dataset=lm_valid_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=3,\n",
    "        per_device_eval_batch_size=3,\n",
    "        logging_dir=logging_dir,\n",
    "        logging_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        bf16=False,\n",
    "        save_strategy=\"no\",\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"tensorboard\",\n",
    "        lr_scheduler_type=lr_schedulers[chosen_scheduler],\n",
    "        factor=0.9,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        tokenizer, \n",
    "        mlm=False\n",
    "    )\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f540a-a100-4da6-914b-593a4e87dec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyLR settings: patience=10 smooth=False min_lr=0.001 factor=0.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/399 02:36 < 3:24:20, 0.03 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.109100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644f108-16f2-47ca-b6ee-dfa14dc24ea5",
   "metadata": {},
   "source": [
    "# Save Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49684591-f1da-4860-a268-cebaad450e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_dir = \"./temp-model-dir\"\n",
    "model_dir = \"./fine-tuned-model-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b572fdf-a5da-4890-9f7e-f1581cb507a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(temp_dir, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5bbae1-7172-484a-aeb8-bc3e6524172c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f5483-527d-4651-bf17-c1a8efbb4b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    temp_dir,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705c830-c304-40ce-878a-64a01dcb2d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    model_dir, \n",
    "    safe_serialization=True, \n",
    "    max_shard_size=\"9GB\"\n",
    ")\n",
    "tokenizer.save_pretrained(\n",
    "    save_directory=model_dir, \n",
    "    from_pt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634adc88-36dd-48a0-b169-3c2819a5328f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442000d4-0a65-4db9-91d8-1c7dcc211261",
   "metadata": {},
   "source": [
    "# Run Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114e5a1-d199-494b-93c6-34db63b85f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./fine-tuned-model-dir\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc23502-68cf-44d3-896a-1d3305810e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./fine-tuned-model-dir\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25690092-637b-46de-96d4-ebf827aae41c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8b5fc-9b59-4d40-b052-d74541cc92a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_simple_cot_test(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['source']}.\"\n",
    "    answer = f\"### Answer\\n\" \n",
    "    # response = f\"### Rationale\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, answer] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb84ed2-bc05-435b-9ad9-876d34dcf40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = format_simple_cot_test(test_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929febf-8922-404c-8dee-4b1e65efb8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e687f91-8cd4-46b8-ac5f-211c9904877a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    sample,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: \\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb3832-c667-4cea-86fa-f277808e8d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8332052-cfac-48d4-a618-4eca7b7657c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e008641-2587-4ac0-a58a-9d070d728bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ed23a8-dbb6-43b8-b5df-cdf2a23d8cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.experiments.experiment import Experiment\n",
    "\n",
    "exp = Experiment.load(experiment_name=\"greedylr-experimentation\", sagemaker_session=sess)\n",
    "exp._delete_all(action=\"--force\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f024b43-7e28-4df0-aa7a-17c66dd5bfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-env:(greedy-lr-py310)",
   "language": "python",
   "name": "greedy-lr-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
